{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "data_list = []\n",
    "# The path containing my datas\n",
    "path_data = './data'\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "# The name of the files\n",
    "link_2014 = \"https://s3.amazonaws.com/tripdata/2014-citibike-tripdata.zip\"\n",
    "link_2015 = \"https://s3.amazonaws.com/tripdata/2015-citibike-tripdata.zip\"\n",
    "link_2016 = \"https://s3.amazonaws.com/tripdata/2016-citibike-tripdata.zip\"\n",
    "link_2017 = \"https://s3.amazonaws.com/tripdata/2017-citibike-tripdata.zip\"\n",
    "link_2018 = \"https://s3.amazonaws.com/tripdata/2018-citibike-tripdata.zip\"\n",
    "link_2019 = \"https://s3.amazonaws.com/tripdata/2019-citibike-tripdata.zip\"\n",
    "link_2020 = \"https://s3.amazonaws.com/tripdata/2020-citibike-tripdata.zip\"\n",
    "link_2021 = \"https://s3.amazonaws.com/tripdata/2021-citibike-tripdata.zip\"\n",
    "link_2022 = \"https://s3.amazonaws.com/tripdata/2022-citibike-tripdata.zip\"\n",
    "link_2023 = \"https://s3.amazonaws.com/tripdata/2023-citibike-tripdata.zip\"\n",
    "\n",
    "\n",
    "file_2014 = \"file_2014.zip\"\n",
    "file_2015 = \"file_2015.zip\"\n",
    "file_2016 = \"file_2016.zip\"\n",
    "file_2017 = \"file_2017.zip\"\n",
    "file_2018 = \"file_2018.zip\"\n",
    "file_2019 = \"file_2019.zip\"\n",
    "file_2020 = \"file_2020.zip\"\n",
    "file_2021 = \"file_2021.zip\"\n",
    "file_2022 = \"file_2022.zip\"\n",
    "file_2023 = \"file_2023.zip\"\n",
    "\n",
    "# List for the couples (file, link)\n",
    "data_list = []\n",
    "\n",
    "data_list.append((file_2014, link_2014))\n",
    "data_list.append((file_2015, link_2015))\n",
    "data_list.append((file_2016, link_2016))\n",
    "data_list.append((file_2017, link_2017))\n",
    "data_list.append((file_2018, link_2018))\n",
    "data_list.append((file_2019, link_2019))\n",
    "data_list.append((file_2020, link_2020))\n",
    "data_list.append((file_2021, link_2021))\n",
    "data_list.append((file_2022, link_2022))\n",
    "data_list.append((file_2023, link_2023))\n",
    "\n",
    "years = range(2014, 2024)\n",
    "\n",
    "for filename,link in data_list:\n",
    "    if os.path.exists(os.path.join(path_data, filename)):\n",
    "        print('The file %s already exists.' % os.path.join(path_data, filename))\n",
    "    else:\n",
    "        r = requests.get(link)\n",
    "        with open(os.path.join(path_data, filename), 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(f'Downloaded file {os.path.join(path_data, filename)}.')\n",
    "# 11 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#def unzip_file(file_path, dest_folder):\n",
    " #   if not os.path.exists(dest_folder):\n",
    "  #      with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "   #         zip_ref.extractall(dest_folder)\n",
    "   #         print(f\"Unzipped: {file_path} to {dest_folder}\")\n",
    "    #else:\n",
    "   #    print(f\"Already unzipped: {file_path}\")\n",
    "#for file_path in data_list:\n",
    " #   unzip_file( f\"{path_data}/{file_path[0]}\", unzipped_folder)\n",
    "\n",
    "\n",
    "## create a folder to store the data\n",
    "#unzip_folder = \"unzip_data\"\n",
    "\n",
    "unzipped_folder = \"unzipped_data\"\n",
    "## the path\n",
    "path_data = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "## list all the files in the data folder with path\n",
    "path_list = [os.path.join(path_data, file) for file in os.listdir(path_data)]\n",
    "#print(path_list)\n",
    "\n",
    "def unzip_files(paths_list, destination_folder):\n",
    "    ## check if the folder exists\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        ## unzip the files\n",
    "        zipfile_list = [file for file in path_list if file.endswith(\".zip\")]\n",
    "        for file in zipfile_list:\n",
    "            with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "                ## test if the file is not already unzipped else prinr a message\n",
    "                if os.path.exists(os.path.join(destination_folder, os.path.basename(file).replace(\".zip\", \"\"))):\n",
    "                    print(f\"{file} is already unzipped\")\n",
    "                else:\n",
    "                    zip_ref.extractall(destination_folder)\n",
    "                    print(f\"Extracted {file} to {destination_folder}\")\n",
    "    else:\n",
    "        print(f\"{destination_folder} alreday exists\")\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "unzip_files(path_list, unzipped_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the path of the first directory. The first directory look like \"2014-citibike-tripdata\"\n",
    "def get_first_directory_path_with_year(year):\n",
    "    return f\"{year}-citibike-tripdata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get all CSV files in the year directory\n",
    "def get_all_csv_files(base_dir):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create 1 dataframe per year by merging all the dataframe per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add year and month for future partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Projet 2 2024\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "years = range(2014, 2024)\n",
    "unzipped_folder = \"unzipped_data\"\n",
    "\n",
    "# We add columns and return df\n",
    "def read_and_process_file(file_path):\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df = df.withColumn(\"file_name\", sf.input_file_name())\n",
    "    df = df.withColumn(\"month\", sf.regexp_extract(\"file_name\", r\"(\\d{4})(\\d{2})-\", 2))\n",
    "    df = df.withColumn(\"year\", sf.regexp_extract(\"file_name\", r\"(\\d{4})(\\d{2})-\", 1))\n",
    "    df = df.drop(\"file_name\")\n",
    "    return df\n",
    "\n",
    "all_df_by_year = []\n",
    "# We create the dataframe per year\n",
    "for year in years:\n",
    "    input_dir = os.path.join(unzipped_folder, get_first_directory_path_with_year(year))\n",
    "    \n",
    "    csv_files = get_all_csv_files(input_dir)\n",
    "    \n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = read_and_process_file(file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Read all CSV files into a single DataFrame\n",
    "    if dfs:\n",
    "        year_df = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            year_df = year_df.unionByName(df, allowMissingColumns=True)\n",
    "        all_df_by_year.append((year, year_df))\n",
    "\n",
    "# 6 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_by_year(all_df,year):\n",
    "    for year_df,df in all_df:\n",
    "        if year_df == year:\n",
    "            return df\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's show all the datas per year and see what we have to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    get_df_by_year( all_df_by_year,year).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that almost all year have different columns name for the same columns and we have some duplicate\n",
    "-   member_casual is the same as usertype but with different value so we will save member_casual and convert usertype value into member_casual ones\n",
    "-   we also have started_at and startime and a lot of columns that says the same thing\n",
    "-   birth year, bike_id and gender disappear after 2021\n",
    "-   ride_id and rideable_type appear after 2021\n",
    "So At the end we will have the same columns with all dataframes and the columns are:\n",
    "rideable_type, started_at,ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat,end_lng, member_casual, month, year, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import  coalesce,lit\n",
    "def process_year_2014_2015_2018_2019_2020(df):\n",
    "    df = (df\\\n",
    "        .withColumnRenamed(\"starttime\", \"started_at\")\\\n",
    "        .withColumnRenamed(\"stoptime\", \"ended_at\")\\\n",
    "        .withColumnRenamed(\"start station id\", \"start_station_id\")\\\n",
    "        .withColumnRenamed(\"start station name\", \"start_station_name\")\\\n",
    "        .withColumnRenamed(\"start station latitude\", \"start_lat\")\\\n",
    "        .withColumnRenamed(\"start station longitude\", \"start_lng\")\\\n",
    "        .withColumnRenamed(\"end station id\", \"end_station_id\")\\\n",
    "        .withColumnRenamed(\"end station name\", \"end_station_name\")\\\n",
    "        .withColumnRenamed(\"end station latitude\", \"end_lat\")\\\n",
    "        .withColumnRenamed(\"end station longitude\", \"end_lng\")\\\n",
    "        .withColumnRenamed(\"usertype\", \"member_casual\")\\\n",
    "        .withColumnRenamed(\"birth year\", \"birth_year\")\\\n",
    "        .drop(\"bikeid\")\n",
    "        .drop(\"tripduration\")\n",
    "        .withColumn(\"rideable_type\", lit(None))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def process_year_2016(df):\n",
    "    # Fusion of duplicate columns\n",
    "    df = (df\n",
    "        .withColumn(\"tripduration\", coalesce(df[\"Trip Duration\"], df[\"tripduration\"]))\n",
    "        .withColumn(\"starttime\", coalesce(df[\"Start Time\"], df[\"starttime\"]))\n",
    "        .withColumn(\"stoptime\", coalesce(df[\"Stop Time\"], df[\"stoptime\"]))\n",
    "        .withColumn(\"bikeid\", coalesce(df[\"Bike ID\"], df[\"bikeid\"]))\n",
    "        .withColumn(\"usertype\", coalesce(df[\"User Type\"], df[\"usertype\"]))\n",
    "        .drop(\"Trip Duration\", \"Start Time\", \"Stop Time\", \"Bike ID\", \"User Type\")\n",
    "    )\n",
    "    \n",
    "    # Changing columns names\n",
    "    df = (df\n",
    "        .withColumnRenamed(\"starttime\", \"started_at\")\n",
    "        .withColumnRenamed(\"stoptime\", \"ended_at\")\n",
    "        .withColumnRenamed(\"Start Station ID\", \"start_station_id\")\n",
    "        .withColumnRenamed(\"Start Station Name\", \"start_station_name\")\n",
    "        .withColumnRenamed(\"Start Station Latitude\", \"start_lat\")\n",
    "        .withColumnRenamed(\"Start Station Longitude\", \"start_lng\")\n",
    "        .withColumnRenamed(\"End Station ID\", \"end_station_id\")\n",
    "        .withColumnRenamed(\"End Station Name\", \"end_station_name\")\n",
    "        .withColumnRenamed(\"End Station Latitude\", \"end_lat\")\n",
    "        .withColumnRenamed(\"End Station Longitude\", \"end_lng\")\n",
    "        .withColumnRenamed(\"usertype\", \"member_casual\")\n",
    "        .withColumnRenamed(\"Birth Year\", \"birth_year\")\\\n",
    "        .withColumnRenamed(\"Gender\", \"gender\")\\\n",
    "        .drop(\"bikeid\")\n",
    "        .drop(\"tripduration\")\n",
    "        .withColumn(\"rideable_type\", lit(None)) \n",
    "    )\n",
    "    return df\n",
    "\n",
    "def process_year_2017(df):\n",
    "\n",
    "    df = (df\n",
    "        .withColumn(\"tripduration\", coalesce(df[\"tripduration\"], df[\"Trip Duration\"]))\n",
    "        .withColumn(\"starttime\", coalesce(df[\"starttime\"], df[\"Start Time\"]))\n",
    "        .withColumn(\"stoptime\", coalesce(df[\"stoptime\"], df[\"Stop Time\"]))\n",
    "        .withColumn(\"bikeid\", coalesce(df[\"bikeid\"], df[\"Bike ID\"]))\n",
    "        .withColumn(\"usertype\", coalesce(df[\"usertype\"], df[\"User Type\"]))\n",
    "        .drop(\"Trip Duration\", \"Start Time\", \"Stop Time\", \"Bike ID\", \"User Type\")\n",
    "    )\n",
    "    \n",
    "    df = (df\n",
    "        .withColumnRenamed(\"starttime\", \"started_at\")\n",
    "        .withColumnRenamed(\"stoptime\", \"ended_at\")\n",
    "        .withColumnRenamed(\"start station id\", \"start_station_id\")\n",
    "        .withColumnRenamed(\"start station name\", \"start_station_name\")\n",
    "        .withColumnRenamed(\"start station latitude\", \"start_lat\")\n",
    "        .withColumnRenamed(\"start station longitude\", \"start_lng\")\n",
    "        .withColumnRenamed(\"end station id\", \"end_station_id\")\n",
    "        .withColumnRenamed(\"end station name\", \"end_station_name\")\n",
    "        .withColumnRenamed(\"end station latitude\", \"end_lat\")\n",
    "        .withColumnRenamed(\"end station longitude\", \"end_lng\")\n",
    "        .withColumnRenamed(\"usertype\", \"member_casual\")\n",
    "        .withColumnRenamed(\"birth year\", \"birth_year\")\n",
    "        .drop(\"bikeid\")\n",
    "        .drop(\"tripduration\")\n",
    "        .withColumn(\"rideable_type\", lit(None)) \n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_year_2021(df):\n",
    "\n",
    "    df = (df\n",
    "        .withColumn(\"started_at\", coalesce(df[\"started_at\"], df[\"starttime\"]))\n",
    "        .withColumn(\"ended_at\", coalesce(df[\"ended_at\"], df[\"stoptime\"]))\n",
    "        .withColumn(\"start_station_name\", coalesce(df[\"start_station_name\"], df[\"start station name\"]))\n",
    "        .withColumn(\"start_station_id\", coalesce(df[\"start_station_id\"], df[\"start station id\"]))\n",
    "        .withColumn(\"end_station_name\", coalesce(df[\"end_station_name\"], df[\"end station name\"]))\n",
    "        .withColumn(\"end_station_id\", coalesce(df[\"end_station_id\"], df[\"end station id\"]))\n",
    "        .withColumn(\"member_casual\", coalesce(df[\"member_casual\"], df[\"usertype\"]))\n",
    "        \n",
    "        .withColumn(\"start_lat\", coalesce(df[\"start_lat\"], df[\"start station latitude\"]))\n",
    "        .withColumn(\"start_lng\", coalesce(df[\"start_lng\"], df[\"start station longitude\"]))\n",
    "        .withColumn(\"end_lat\", coalesce(df[\"end_lat\"], df[\"end station latitude\"]))\n",
    "        .withColumn(\"end_lng\", coalesce(df[\"end_lng\"], df[\"end station longitude\"]))\n",
    "        .drop(\"ride_id\",\"tripduration\", \"starttime\", \"stoptime\", \"start station id\", \"start station name\", \"start station latitude\", \"start station longitude\", \"end station id\", \"end station name\", \"end station latitude\", \"end station longitude\", \"bikeid\", \"usertype\")\n",
    "    )\n",
    "    \n",
    "    df = df.withColumnRenamed(\"birth year\", \"birth_year\")\n",
    "    return df\n",
    "\n",
    "def process_year_2022_2023(df):\n",
    "    df = (df\n",
    "        .withColumn(\"birth_year\", lit(None))\n",
    "        .withColumn(\"gender\", lit(None))\n",
    "        .drop(\"ride_id\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Process data per year to have the same columns after\n",
    "def process_using_year(df, year):\n",
    "    if year in [2014, 2015, 2018, 2019, 2020]:\n",
    "        return process_year_2014_2015_2018_2019_2020(df)\n",
    "    elif year == 2016:\n",
    "        return process_year_2016(df)\n",
    "    elif year == 2017:\n",
    "        return process_year_2017(df)\n",
    "    elif year == 2021:\n",
    "        return process_year_2021(df)\n",
    "    elif year in [2022, 2023]:\n",
    "        return process_year_2022_2023(df)\n",
    "    else:\n",
    "        print(\"Year {} not supported.\".format(year))\n",
    "        return df  # Retourne le DataFrame non modifié si l'année n'est pas prise en charge\n",
    "\n",
    "# Adapting usertype values to member_casual values\n",
    "def process_member_casual(df):\n",
    "    df = df.withColumn(\"member_casual\", \n",
    "        when(df[\"member_casual\"] == \"Subscriber\", \"member\")\n",
    "        .when(df[\"member_casual\"] == \"Customer\", \"casual\")\n",
    "        .otherwise(df[\"member_casual\"])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "input_date_format = 'MM/dd/yyyy HH:mm:ss'\n",
    "\n",
    "output_date_format = 'yyyy-MM-dd HH:mm:ss'\n",
    "\n",
    "from pyspark.sql.types import TimestampType\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Changing all dates into timestamp\n",
    "def process_timestamp_format(df):\n",
    "    # Getting the types\n",
    "    started_at_type = df.select(\"started_at\").dtypes[0][1]\n",
    "    ended_at_type = df.select(\"ended_at\").dtypes[0][1]\n",
    "    \n",
    "    if started_at_type != 'timestamp':\n",
    "        # print(f\"Pas timeStamp mais {started_at_type}\")\n",
    "        df = df.withColumn('started_at', to_timestamp(df['started_at'], input_date_format))\n",
    "    if ended_at_type != 'timestamp':\n",
    "        df = df.withColumn('ended_at', to_timestamp(df['ended_at'], input_date_format))\n",
    "    \n",
    "    return df\n",
    "\n",
    "column_types = {\n",
    "    'birth_year': 'string',\n",
    "    'gender': 'string', \n",
    "    'rideable_type': 'string', \n",
    "    'started_at': 'timestamp', \n",
    "    'ended_at': 'timestamp',\n",
    "    'start_station_name': 'string', \n",
    "    'start_station_id': 'string',\n",
    "    'end_station_name': 'string', \n",
    "    'end_station_id': 'string',\n",
    "    'start_lat': 'double', \n",
    "    'start_lng': 'double', \n",
    "    'end_lat': 'double',\n",
    "    'end_lng': 'double', \n",
    "    'member_casual': 'string', \n",
    "    'month': 'integer', \n",
    "    'year': 'integer'\n",
    "}\n",
    "\n",
    "# Applying types\n",
    "def apply_column_types(df):\n",
    "    for column, data_type in column_types.items():\n",
    "        df = df.withColumn(column, df[column].cast(data_type))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (year, year_df) in enumerate(all_df_by_year):\n",
    "    print(f\"Annee {year}\")\n",
    "    year_df = process_using_year(year_df, year)\n",
    "    year_df = process_member_casual(year_df)\n",
    "    year_df = process_timestamp_format(year_df)\n",
    "    year_df = apply_column_types(year_df)\n",
    "    \n",
    "    all_df_by_year[i] = (year,year_df)\n",
    "    \n",
    "# sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = [set(df.columns) for _, df in all_df_by_year]\n",
    "\n",
    "# Just checking if all columns names are good\n",
    "if all(column_set == column_sets[0] for column_set in column_sets):\n",
    "    print(\"Same columns name\")\n",
    "else:\n",
    "    print(\"Not Same columns name\")\n",
    "\n",
    "# Checking if all dataframes have the same amount of columns\n",
    "num_columns = len(all_df_by_year[0][1].columns)\n",
    "if all(len(df.columns) == num_columns for _, df in all_df_by_year):\n",
    "    print(\"Same number of column\")\n",
    "else:\n",
    "    print(\"Not Same number of columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheking the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year,year_df in all_df_by_year:\n",
    "    year_df.select('started_at', 'ended_at').show(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_by_year( all_df_by_year,2019).count(),get_df_by_year( all_df_by_year,2023).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici c'est juste pour chercher les null d'un df\n",
    "df = get_df_by_year( all_df_by_year,2019)\n",
    "df.select([sf.count(sf.when(sf.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "# 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = get_df_by_year( all_df_by_year,2014)\n",
    "df_2023 = get_df_by_year( all_df_by_year,2023)\n",
    "df_2021 = get_df_by_year( all_df_by_year,2021)\n",
    "df_2020 = get_df_by_year( all_df_by_year,2020)\n",
    "\n",
    "f_df14 = df_2014.filter(df_2014[\"start_station_name\"] == \"Suffolk St & Stanton St\")\n",
    "f_df23 = df_2023.filter(df_2023[\"start_station_name\"] == \"Suffolk St & Stanton St\")\n",
    "f_df21 = df_2021.filter(df_2021[\"start_station_name\"] == \"Suffolk St & Stanton St\")\n",
    "f_df20 = df_2020.filter(df_2020[\"start_station_name\"] == \"Suffolk St & Stanton St\")\n",
    "\n",
    "# Results\n",
    "f_df14.show(1),f_df21.show(1),f_df20.show(1),f_df23.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see statiosn have different ids starting at 2022 and slightly changes on latitude and longitude but we won't touch it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's union all dataframes to have only one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_all_dfs_by_name(all_df_by_year):\n",
    "    merged_df = all_df_by_year[0][1]\n",
    "    for _, df in all_df_by_year[1:]:\n",
    "        merged_df = merged_df.unionByName(df)\n",
    "    return merged_df\n",
    "\n",
    "merged_df = union_all_dfs_by_name(all_df_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.show(2),merged_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to parquet using column year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## partioning our data with a range partition of years and months\n",
    "\n",
    "\n",
    "## lets have a vieuw of the data mergeddf   \n",
    "merged_df.printSchema()\n",
    "merged_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patrtioning the data by year and month\n",
    "#years = range(2014, 2024)\n",
    "\n",
    "#months = range(1, 13)\n",
    "\n",
    "# Partitioning the data by year \n",
    "#merged_df = merged_df.repartition(\"years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## the actual number of partitions for the merged_df\n",
    "num_partitions = merged_df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions: \", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## lets have the numer of rows and columns in the merged_df\n",
    "num_rows = merged_df.count()\n",
    "num_cols = len(merged_df.columns)\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"./trip_data.parquet\"\n",
    "# parquet_path = \"./parquet_data\"\n",
    "partition_columns = [\"year\", \"month\"]\n",
    "if not os.path.exists(parquet_path):\n",
    "    merged_df.write.partitionBy(*partition_columns).parquet(parquet_path)\n",
    "    print(f\"Data written to {parquet_path}\")\n",
    "else:\n",
    "    print(f\"Path {parquet_path} already exists. No data written.\")\n",
    "    \n",
    "#merged_df.write.partitionBy(*partition_columns)\n",
    "\n",
    "# 40 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.unpersist()\n",
    "# for year,df in all_df_by_year:\n",
    "#     df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Javascript\n",
    "\n",
    "# def restart_kernel():\n",
    "#     display(Javascript('IPython.notebook.kernel.restart()'))\n",
    "\n",
    "# restart_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage level Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Storage level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We can easily tune the number of partiton with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` spark.read.option(\"numPartitions\", the_number_we_want).csv(\"path/to/csv\") ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Why modify the number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we modify this number to optimize the parallelism of the jobs, ensuring that all executors are effectively utilized according o the size of the data. Ex: With small datas, its a bad idea to have a lot of partitions. But with large dataset it performs very good with lot of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing into star Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's restart spark first\n",
    "We had a lot of memory problems so we just restart spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Projet 2 2024\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 100) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "# .config(\"spark.memory.storageFraction\", \"0.3\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"./trip_data.parquet\"\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "parquet_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "start_stations = parquet_df.select(col(\"start_station_id\").alias(\"station_id\"),\n",
    "    col(\"start_station_name\").alias(\"station_name\")).dropDuplicates([\"station_id\"])\n",
    "end_stations = parquet_df.select(col(\"end_station_id\").alias(\"station_id\"),\n",
    "    col(\"end_station_name\").alias(\"station_name\")).dropDuplicates([\"station_id\"])\n",
    "\n",
    "station_id_name = start_stations.union(end_stations).dropDuplicates([\"station_id\"])\n",
    "\n",
    "start_coordinates = parquet_df.select(col(\"start_station_id\").alias(\"station_id\"),\n",
    "    col(\"start_lat\").alias(\"lat\"),\n",
    "    col(\"start_lng\").alias(\"lng\")).dropDuplicates([\"station_id\"])\n",
    "end_coordinates = parquet_df.select(col(\"end_station_id\").alias(\"station_id\"),\n",
    "    col(\"end_lat\").alias(\"lat\"),\n",
    "    col(\"end_lng\").alias(\"lng\")).dropDuplicates([\"station_id\"])\n",
    "\n",
    "station_coordinates = start_coordinates.dropDuplicates([\"station_id\"])\n",
    "\n",
    "station_df = station_id_name.join(station_coordinates, on=\"station_id\", how=\"inner\") \\\n",
    "    .select(\"station_id\", \"station_name\", \"lat\", \"lng\")\n",
    "# start_stations.unpersist()\n",
    "# end_stations.unpersist()\n",
    "# start_coordinates.unpersist()\n",
    "# end_coordinates.unpersist()\n",
    "# import gc\n",
    "\n",
    "# del start_stations\n",
    "# del end_stations\n",
    "# del start_coordinates\n",
    "# del end_coordinates\n",
    "\n",
    "# # Force garbage collector à free memory\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trip information for the event table\n",
    "trips_df = parquet_df.select(\n",
    "    \"started_at\",\n",
    "    \"ended_at\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_id\",\n",
    "    \"rideable_type\",\n",
    "    \"member_casual\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"gender\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.persist()\n",
    "#2 min 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing and showing plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the dataset  because of space problems and it take too many times to compute (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from geopy.distance import geodesic\n",
    "import plotly.express as px\n",
    "\n",
    "# plot_df = parquet_df.dropna(subset=[\"started_at\"])\n",
    "# plot_df = plot_df.dropna(subset=[\"ended_at\"])\n",
    "\n",
    "def calculate_distance(start_lat, start_lng, end_lat, end_lng):\n",
    "    start_coords = (start_lat, start_lng)\n",
    "    end_coords = (end_lat, end_lng)\n",
    "    return geodesic(start_coords, end_coords).kilometers\n",
    "\n",
    "# Just une lambda and we force type to be Double\n",
    "distance_udf = udf(calculate_distance, DoubleType())\n",
    "\n",
    "def trip_distance_each_day_of_week(year_df):\n",
    "    \n",
    "    plot_df = year_df.withColumn(\"trip_distance_km\", distance_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\n",
    "\n",
    "    plot_df = plot_df.withColumn(\"day_of_week\", date_format(col(\"started_at\"), \"EEEE\"))\n",
    "    \n",
    "    plot_df = plot_df.select(\"trip_distance_km\", \"day_of_week\")\n",
    "    \n",
    "    plot_df_pandas = plot_df.limit(150000).toPandas()  # Limiting for performance\n",
    "\n",
    "    fig = px.histogram(plot_df_pandas, x=\"day_of_week\", y=\"trip_distance_km\", \n",
    "        title='Trip Distance by Day of Week',\n",
    "        labels={'trip_distance_km': 'Trip Distance (km)', 'day_of_week': 'Day of the Week'})\n",
    "\n",
    "    fig.update_layout(xaxis={'categoryorder':'array', 'categoryarray':['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip distribution each day of week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distribution pre-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = parquet_df.filter(parquet_df['year'] == 2018)\n",
    "trip_distance_each_day_of_week(df_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we limited the dataset so we are missing some days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip distribution post-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = parquet_df.filter(parquet_df['year'] == 2022)\n",
    "trip_distance_each_day_of_week(df_2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is better distributed past-covid, we can see modr distance the closer we are to the weekend and it decrease as the weekend is ending. The post-covid ones does not telle us much be cause we dont have all the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add count columns\n",
    "def trip_per_couple_location(df_year):\n",
    "    plot_df = df_year.groupBy(\"start_station_name\", \"end_station_name\").count()\n",
    "    plot_df.limit(150000)\n",
    "    plot_df = plot_df.toPandas()\n",
    "    pd = plot_df\n",
    "    import plotly.express as px\n",
    "\n",
    "    import plotly.graph_objects as go\n",
    "    plot_df = plot_df.pivot(index='start_station_name', columns='end_station_name', values='count')\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=plot_df.values,\n",
    "        x=plot_df.columns,\n",
    "        y=plot_df.index,\n",
    "        colorscale='Viridis'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Trip Counts Heatmap for Start and End Station Pairs',\n",
    "        xaxis=dict(title='End Station'),\n",
    "        yaxis=dict(title='Start Station'),\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trips per pickup/dropoff location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trips per pickup/dropoff location prec - covid (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_per_couple_location(df_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trips per pickup/dropoff location post - covid (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_per_couple_location(df_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to read but it's the best of all we tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gender(gender):\n",
    "    if gender == '1':\n",
    "        return 'Male'\n",
    "    elif gender == '2':\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "map_gender_udf = udf(map_gender, StringType())\n",
    "\n",
    "def trip_distance_distribution_by_gender(df):\n",
    "    df = df.withColumn('gender_label', map_gender_udf(col('gender')))\n",
    "\n",
    "    df = df.withColumn(\"trip_distance_km\", distance_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\n",
    "    df = df.groupBy('gender_label').agg({'trip_distance_km': 'sum'})\n",
    "    df_pd = df.toPandas()\n",
    "\n",
    "    fig = px.bar(df_pd, x='gender_label', y='sum(trip_distance_km)', title='Total Trip Distance by Gender')\n",
    "    fig.update_layout(xaxis_title='Gender', yaxis_title='Total Trip Distance (km)')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip distance distribution per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = parquet_df.filter(parquet_df['year'] == 2020)\n",
    "plot_df = df_2020.limit(70000)\n",
    "trip_distance_distribution_by_gender(plot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip distribution for age ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import radians, sin, cos, sqrt, asin, col\n",
    "\n",
    "def calculate_distance2(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    # Formula to calculate distance\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    distance = 2 * asin(sqrt(a)) * 6371  # Radius of earth in kilometers\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import year, current_date\n",
    "# def calculate_age(birth_year):\n",
    "#     current_year = datetime.datetime.now().year\n",
    "#     return current_year - int(birth_year)\n",
    "\n",
    "def assign_age_range(age):\n",
    "    if age >= 15 and age <= 24:\n",
    "        return '15-24'\n",
    "    elif age >= 25 and age <= 44:\n",
    "        return '25-44'\n",
    "    elif age >= 45 and age <= 54:\n",
    "        return '45-54'\n",
    "    elif age >= 55 and age <= 64:\n",
    "        return '55-64'\n",
    "    else:\n",
    "        return '65+'\n",
    "\n",
    "\n",
    "#df_2020_age = df_2020.withColumn('age', year(current_date()) - df_2020['birth_year'])\n",
    "#assign_age_range_udf = udf(assign_age_range, StringType())\n",
    "#df_with_age_range = df_2020_age.withColumn('age_range', assign_age_range_udf(col('age')))\n",
    "#df_with_trip_distance = df_with_age_range.withColumn('trip_distance_km', calculate_distance2(col('start_lat'), col('start_lng'), col('end_lat'), col('end_lng')))\n",
    "#trip_distance_distribution = df_with_trip_distance.groupBy('age_range').agg({'trip_distance_km': 'collect_list'})\n",
    "\n",
    "def trip_distance_distribution_by_age(df):\n",
    "    df_with_age = df.withColumn('age', year(current_date()) - df_2020['birth_year'])\n",
    "\n",
    "    assign_age_range_udf = udf(assign_age_range, StringType())\n",
    "    df_with_age_range = df_with_age.withColumn('age_range', assign_age_range_udf(col('age')))\n",
    "\n",
    "    # df_with_trip_distance = df_with_age_range.withColumn('trip_distance_km', calculate_distance2(df_with_age_range['start_lat'], df_with_age_range['start_lng'], df_with_age_range['end_lat'], df_with_age_range['end_lng']))\n",
    "    df_with_trip_distance = df_with_age_range.withColumn('trip_distance_km', distance_udf(col('start_lat'), col('start_lng'), col('end_lat'), col('end_lng')))\n",
    "    trip_distance_distribution = df_with_trip_distance.groupBy('age_range').agg({'trip_distance_km': 'collect_list'})\n",
    "    return trip_distance_distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def visualize_trip_distance_distribution(trip_distance_distribution):\n",
    "    trip_distance_distribution_pd = trip_distance_distribution.toPandas()\n",
    "\n",
    "    fig = px.box(trip_distance_distribution_pd, x='age_range', y='collect_list(trip_distance_km)',\n",
    "        title='Trip Distance Distribution by Age Range',\n",
    "        labels={'age_range': 'Age Range', 'collect_list(trip_distance_km)': 'Trip Distance (km)'})\n",
    "\n",
    "    fig.update_layout(xaxis_title='Age Range', yaxis_title='Trip Distance (km)')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri= trip_distance_distribution_by_age(df_2020.limit(1000))\n",
    "tri.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip_distance_distribution = trip_distance_distribution_by_age(df_2020.limit(1000))\n",
    "\n",
    "# visualize_trip_distance_distribution(trip_distance_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance_distribution = trip_distance_distribution_by_age(df_2020.limit(150000))\n",
    "def visualize_trip_distance_distribution_histogram(trip_distance_distribution):\n",
    "    trip_distance_distribution_pd = trip_distance_distribution.toPandas()\n",
    "\n",
    "    trip_distance_distribution_pd = trip_distance_distribution_pd.explode('collect_list(trip_distance_km)')\n",
    "\n",
    "    fig = px.histogram(trip_distance_distribution_pd, x='collect_list(trip_distance_km)', color='age_range',\n",
    "        title='Trip distribution by age range',\n",
    "        labels={'collect_list(trip_distance_km)': 'Distance du Trajet (km)', 'age_range': 'Age range'},\n",
    "        nbins=50, barmode='overlay')\n",
    "\n",
    "    fig.update_layout(xaxis_title='Distance du Trajet (km)', yaxis_title='Fréquence')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "visualize_trip_distance_distribution_histogram(trip_distance_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to what one might think 25-44 age range uses the bikes more than 15-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_distance_distribution_by_rideable_type(df):\n",
    "    df = df.filter(df['rideable_type'].isNotNull())\n",
    "\n",
    "    df = df.withColumn(\"trip_distance_km\", distance_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\n",
    "\n",
    "    df = df.groupBy('rideable_type').agg({'trip_distance_km': 'sum'})\n",
    "    df.limit(100)\n",
    "    fig = px.bar(df.toPandas(), x='rideable_type', y='sum(trip_distance_km)', title='Total Trip Distance by bike type')\n",
    "    fig.update_layout(xaxis_title='bike type', yaxis_title='Total Trip Distance (km)')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post covid trip distribution for bikes types(non null): 2021  2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023= parquet_df.filter(parquet_df['year'] == 2023)\n",
    "df_trip_bike_2023 = trip_distance_distribution_by_rideable_type(df_2023.limit(15000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after covid we can see that the distribution is unbanlanced, we have a lot of classic bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## During covid trip distribution for bikes types (non null): 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021= parquet_df.filter(parquet_df['year'] == 2021)\n",
    "df_trip_bike_2021 = trip_distance_distribution_by_rideable_type(df_2021.limit(15000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trip distance was very high during covid compared to 2023 (actually weird). Possible raisons are the transport were shut down during covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing seasonalities and looking at time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The number of pickups/docks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofweek, hour, count, date_format\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = parquet_df.withColumn('day_of_week', date_format(col('started_at'), 'E'))\n",
    "df = df.withColumn('hour_of_day', hour(col('started_at')))\n",
    "\n",
    "grouped_df = df.groupBy('day_of_week', 'hour_of_day').agg(count('*').alias('pickups_docks'))\n",
    "\n",
    "pandas_df = grouped_df.toPandas()\n",
    "\n",
    "ordered_days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "pandas_df['day_of_week'] = pd.Categorical(pandas_df['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "pivot_df = pandas_df.pivot_table(index='hour_of_day', columns='day_of_week', values='pickups_docks', fill_value=0)\n",
    "pivot_df = pivot_df.astype(int)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_df, cmap='viridis', annot=True, fmt='d')\n",
    "plt.title('Number of Pickups/Docks by Day of the Week and Hour of the Day')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of the Day')\n",
    "plt.show()\n",
    "#   1 - 6min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have guessed, we have more trips during workdays and hours like 8am (when people goes to work) and 4-6 pm (when people gets off of work). Notice that here we used the dataframe of all the years because it does not long long time compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The average distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_2022.limit(550000)\n",
    "df = df.withColumn(\"distance_km\", distance_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\n",
    "df = df.withColumn('day_of_week', date_format(col('started_at'), 'E'))\n",
    "df = df.withColumn('hour_of_day', hour(col('started_at'))) \n",
    "\n",
    "df_avg_distance = df.groupBy('day_of_week', 'hour_of_day').agg(mean('distance_km').alias('avg_distance_km'))\n",
    "\n",
    "pandas_avg_distance = df_avg_distance.toPandas()\n",
    "\n",
    "pandas_avg_distance['day_of_week'] = pd.Categorical(pandas_avg_distance['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "pivot_avg_distance = pandas_avg_distance.pivot_table(index='hour_of_day', columns='day_of_week', values='avg_distance_km', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_avg_distance, cmap='viridis', annot=True, fmt='.2f')\n",
    "plt.title('Average Distance by Day of the Week and Hour of the Day')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of the Day')\n",
    "plt.show()\n",
    "# 2 min - 10 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty normal beside the weird peak on tuesday and saturday morning at 3 am. Of course we lilmited the data be cause of long loading time crashing the kernel so it's possible that those peaks are biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The average trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_2022.limit(550000)\n",
    "df = df.withColumn(\"distance_km\", distance_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\n",
    "\n",
    "df = df.withColumn('trip_duration', unix_timestamp('ended_at') - unix_timestamp('started_at'))\n",
    "df = df.withColumn('trip_duration', col('trip_duration') / 60)\n",
    "df = df.withColumn('day_of_week', date_format(col('started_at'), 'E')) \n",
    "df = df.withColumn('hour_of_day', hour(col('started_at'))) \n",
    "\n",
    "df_avg_duration = df.groupBy('day_of_week', 'hour_of_day').agg(mean('trip_duration').alias('avg_trip_duration'))\n",
    "\n",
    "pandas_avg_duration = df_avg_duration.toPandas()\n",
    "\n",
    "pandas_avg_duration['day_of_week'] = pd.Categorical(pandas_avg_duration['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "pivot_avg_duration = pandas_avg_duration.pivot_table(index='hour_of_day', columns='day_of_week', values='avg_trip_duration', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_avg_duration, cmap='viridis', annot=True, fmt='.2f')\n",
    "plt.title('Average Trip Duration by Day of the Week and Hour of the Day in minutes')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of the Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have long trips duration on weekend probably because people who use bikes on work days to go to work don't take long trips. Either the work office is close so they use the bike for smal trip duration or the use the bike just to get to the station and take transport so small trip duration too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of ongoing trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, hour, date_format, count, mean\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = df_2022.limit(5000000)\n",
    "df = df.withColumn('day_of_week', date_format(col('started_at'), 'E'))\n",
    "df = df.withColumn('hour_of_day', hour(col('started_at')))\n",
    "df = df.withColumn('date', to_date(col('started_at')))\n",
    "\n",
    "# Partition so we add to each row a colums that give number of trips for that day\n",
    "windowSpec = Window.partitionBy('date', 'hour_of_day')\n",
    "df = df.withColumn('num_trips', count('started_at').over(windowSpec))\n",
    "df = df.groupBy('hour_of_day', 'day_of_week').agg(mean('num_trips').alias('avg_trips'))\n",
    "\n",
    "pandas_avg_trips = df.toPandas()\n",
    "\n",
    "pandas_avg_trips['day_of_week'] = pd.Categorical(pandas_avg_trips['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "pivot_avg_departures = pandas_avg_trips.pivot_table(index='hour_of_day', columns='day_of_week', values='avg_trips', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_avg_departures, cmap='viridis', annot=True, fmt='.2f')\n",
    "plt.title('Average Number of trips by Day of the Week and Hour of the Day')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of the Day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It cannot get more clear than that, rush hours of work days have a lot of trips. So does the weekend !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor job execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Analysing Analyzed and Optimized plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between analyzed and optimized logical plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The Optimized Logical Plan is  samller\n",
    "-   The Optimized Logical Plan  applies the limit we did for the dataframes earlier compared to Analyzed Logical Plan\n",
    "-   The Optimized Logical Plan retained only necessary columns\n",
    "-   and more ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDBMS\n",
    "Spark would perform a little bit better than RDBMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Analysing physical and Optimized plan\n",
    "The physical logical plan specifies the actual methods that will be used to process the data such as join shuffles etc ...  It also uses not RDBMS keywords like AdaptiveSparkPlan (An optimized plan use by spark based on runtime statistics), Project(Selection of columns from dataframe ), Sort (is order by in RDBMS),Exchange, ColumnarToRow etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Inspecting spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_2022.limit(5000000)\n",
    "df = df.withColumn('day_of_week', date_format(col('started_at'), 'E'))\n",
    "df = df.withColumn('hour_of_day', hour(col('started_at')))\n",
    "df = df.withColumn('date', to_date(col('started_at')))\n",
    "\n",
    "windowSpec = Window.partitionBy('date', 'hour_of_day')\n",
    "df = df.withColumn('num_trips', count('started_at').over(windowSpec))\n",
    "df = df.groupBy('hour_of_day', 'day_of_week').agg(mean('num_trips').alias('avg_trips'))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the UI, we had 2 stages. Hash agrregates is used to group large datasets by value. Hash partitioning shufffles and repartition the data whil ensuring data with same keys ends up on the same node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shuffle operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 shuffle  (Exchange) int the physical plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. Stages and tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stages are created based on shuffle ans whithin each stage we have tasks that represents the parallel execution unitsof a partition of the data. Our first stage has 16 tasks and the second one have 1 task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving into spatial information problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Map of trips betweeen stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from geopy.distance import geodesic\n",
    "# import plotly.express as px\n",
    "df_2019 = parquet_df.filter(parquet_df['year'] == 2019)\n",
    "df_filtered = df_2022.limit(7000000)\n",
    "# Agrégation per stations couple\n",
    "station_pairs = df_filtered.groupBy(\"start_station_name\", \"start_lat\", \"start_lng\", \"end_station_name\", \"end_lat\", \"end_lng\") \\\n",
    "    .agg(count(\"*\").alias(\"trip_count\"))\n",
    "station_pairs_pd = station_pairs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString\n",
    "import contextily as ctx\n",
    "import numpy as np\n",
    "\n",
    "# Geopanda need gemetry column so we add \n",
    "station_pairs_pd['geometry'] = station_pairs_pd.apply(\n",
    "    lambda row: LineString([(row['start_lng'], row['start_lat']), (row['end_lng'], row['end_lat'])]), axis=1)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(station_pairs_pd, geometry='geometry')\n",
    "# CRS)\n",
    "gdf.crs = \"EPSG:4326\"\n",
    "gdf = gdf.to_crs(epsg=3857)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot map\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# plot lines\n",
    "gdf.plot(ax=ax, column='trip_count', cmap='viridis', linewidth=1, legend=True)\n",
    "\n",
    "# base map\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, reset_extent=False)\n",
    "ax.autoscale()\n",
    "\n",
    "plt.title('Heatmap of trips at New York')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map does not show that much of yellow color but we can clearly see from the select under that we have some couple of stations with trips over 1500. They are just stuck behind the violet ones. (DATA from 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered = gdf[gdf['trip_count'] > 1500]\n",
    "gdf_filtered.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Interactive Map of trips betweeen stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "station_pandas_df = station_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame from the Pandas DataFrame\n",
    "# DataFrame with 'lng' and 'lat' columns\n",
    "geometry = [Point(xy) for xy in zip(station_pandas_df.lng, station_pandas_df.lat)]\n",
    "geo_station_df = GeoDataFrame(station_pandas_df, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_station_df.to_file(\"output.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2019 = parquet_df.filter(col(\"year\") == 2019).limit(5000000)\n",
    "# trip_counts = df_2019.groupBy(\"start_station_id\", \"end_station_id\") \\\n",
    "#     .count() \\\n",
    "#     .withColumnRenamed(\"count\", \"trip_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip_counts_pd = trip_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Load the station data\n",
    "geo_station_df = gpd.read_file(\"output.geojson\")\n",
    "\n",
    "# # Merge trip counts with station data\n",
    "# trip_counts_merged = trip_counts_pd.merge(geo_station_df, left_on=\"start_station_id\", right_on=\"station_id\")\n",
    "# trip_counts_merged = trip_counts_merged.merge(geo_station_df, left_on=\"end_station_id\", right_on=\"station_id\", suffixes=('_start', '_end'))\n",
    "\n",
    "# # Create the heatmap indexed by start and end stations where the color represents the number of trips count\n",
    "# fig = px.density_mapbox(trip_counts_merged, lat='lat_start', lon='lng_start', z='trip_count', radius=10,\n",
    "#     center=dict(lat=40.7128, lon=-74.0060), zoom=10,\n",
    "#     mapbox_style=\"open-street-map\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, hour as extract_hour\n",
    "\n",
    "## hours from timestamp\n",
    "df_2019 = df_2019.withColumn(\"hour\", extract_hour(col(\"started_at\")))\n",
    "\n",
    "## groupz by start and end stations and hour to get trip counts\n",
    "hourly_trip_counts = df_2019.groupBy(\"start_station_id\", \"end_station_id\", \"hour\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"trip_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# load in geojson file station\n",
    "geo_station_df = gpd.read_file(\"output.geojson\")\n",
    "\n",
    "geo_df = gpd.read_file(\"output.geojson\")\n",
    "\n",
    "# Convert stations data to DataFrame and rename columns for merging\n",
    "stations_dfs = geo_df[['station_id', 'geometry']]\n",
    "stations_dfs['latitude'] = geo_df.geometry.y\n",
    "stations_dfs['longitude'] = geo_df.geometry.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "stations_pds = stations_dfs[['station_id', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "hourly_trip_counts_pd = hourly_trip_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge trip counts with start and end station coordinates\n",
    "hourly_trip_counts_merged = hourly_trip_counts_pd.merge(stations_pds, left_on=\"start_station_id\", right_on=\"station_id\")\n",
    "hourly_trip_counts_merged = hourly_trip_counts_merged.merge(stations_pds, left_on=\"end_station_id\", right_on=\"station_id\", suffixes=('_start', '_end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a single DataFrame to store all the traces\n",
    "all_traces = []\n",
    "\n",
    "for hr in range(24):\n",
    "    hourly_data = hourly_trip_counts_merged[hourly_trip_counts_merged['hour'] == hr]\n",
    "    trace = go.Densitymapbox(\n",
    "        lat=hourly_data['latitude_start'],\n",
    "        lon=hourly_data['longitude_start'],\n",
    "        z=hourly_data['trip_count'],\n",
    "        radius=10,\n",
    "        name=f'Hour {hr}',\n",
    "        visible=(hr == 0)  # Only the first hour is visible initially\n",
    "    )\n",
    "    all_traces.append(trace)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "# Update layout for mapbox\n",
    "fig.update_layout(\n",
    "    mapbox=dict(\n",
    "        style=\"open-street-map\",\n",
    "        center=dict(lat=40.7128, lon=-74.0060),\n",
    "        zoom=10\n",
    "    ),\n",
    "    updatemenus=[{\n",
    "        \"buttons\": [\n",
    "            {\n",
    "                \"args\": [{\"visible\": [hr == i for i in range(24)]}],\n",
    "                \"label\": f\"Hour {hr}\",\n",
    "                \"method\": \"update\"\n",
    "            } for hr in range(24)\n",
    "        ],\n",
    "        \"direction\": \"down\",\n",
    "        \"showactive\": True,\n",
    "    }],\n",
    "    sliders=[{\n",
    "        \"steps\": [\n",
    "            {\n",
    "                \"args\": [\n",
    "                    [hr],\n",
    "                    {\"frame\": {\"duration\": 500, \"redraw\": True}, \"mode\": \"immediate\"}\n",
    "                ],\n",
    "                \"label\": str(hr),\n",
    "                \"method\": \"animate\"\n",
    "            } for hr in range(24)\n",
    "        ],\n",
    "        \"transition\": {\"duration\": 0},\n",
    "        \"x\": 0.1,\n",
    "        \"len\": 0.9\n",
    "    }]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
